# DP-TensorflowPrivacy

Why Differentially privacy?

These days, companies are using more and more of our data to improve their products and services, and it makes a lot of sense if you think about it, it's better to measure what your users like than to guess and build products that no one wants to use. However, this is also very dangerous, it undermines our privacy because the collected data can be quite sensitive, causing harm if it would leak. So companies love data to improve their products but we as users, we want to protect our privacy. These contradicting needs can be satisfied with a technique called differential privacy. 

  It allows companies to collect information about their users without compromising the privacy of the individual. But let's first take a look at why we wouldn't go through all this trouble, companies can just take our data remove our names and call it a day. Right. Well, not quite. First of all this anonymization process usually happens on the servers of the companies that collect your data. So you have to trust them to really remove the identifiable records. And secondly, how Anonymous is anonymized data really, in 2006, Netflix started a competition called the Netflix price, competing teams had to create an algorithm that could predict how someone rated a movie. To help with this challenge, Netflix provided a data set containing over 100 million ratings submitted by over 480,000 users, for more than 17,000 movies, Netflix, of course anonymize this data set by removing the names of users, and by replacing some ratings with fake and random ratings, even though that sounds pretty anonymous it actually wasn't to computer scientists from the University of Texas, published a paper in 2008 that said that they had successfully identified people from this dataset. By combining it with data from IMDB.

  These types of attacks are called linkage attacks, and it happens when pieces of seemingly anonymous data can be combined to reveal real identities or not another more creepy example would be the case of the governor of Massachusetts in the mid 1990s, the state's Group Insurance Commission decided to publish the hospital visits of state employees. They anonymize this data by removing names addresses and other fields that could identify people. However, computer scientists LaTanya Sweeney decided to show how easy it was to reverse this, she combined the published health records, with voter registration records, and simply reduced the list. There was only one person in the medical data that lived in the same zip code, had the same gender, and the same date of birth, as the governor, thus exposing his medical records. In a later paper she noted that 87% of all Americans can be identified with only three pieces of information, zip code, birthday, and gender. So much for anonymity.
  
  Clearly this technique isn't enough to protect our privacy differential privacy on the other hand, neutralizes these types of attacks to explain how it works, let's assume that we want to get a view on how many people do something embarrassing, like for example picking their nose. To do that, we set up a service with the question, do you pick your nose, and with the SN no buttons below it, we collect all these answers on a server somewhere, but instead of sending the real answers, we're going to introduce some noise. Let's say that Bob has a nose speaker and that he clicks on the Yes button before we send his response to the server, our differential privacy algorithm could flip a coin. If it's heads, the algorithm sends Bob's real answer to our server. If it steals the algorithm flips a second coin and sends yes if it steals or no if it's heads back on our server, we see the data coming in but because of the added noise we can't really trust individual records or record for Bob might say that he's in those bigger, but there is at least the one in four chance that he's actually not a no speaker, but that the answer was simply the effect of the coin toss that the algorithm performs. This is plausible deniability. You can be sure of people's answers, so you can judge them on it. This is particularly interesting, if you're collecting data about illegal behavior, such as drug use, for instance. Now because you know how the noise is distributed, you can compensate for it, and end up with a fairly accurate view on how many people are actually no speakers. Now of course, the coin toss algorithm is just an example and a bit too simple, real world algorithms use the Laplace distribution to spread data over a larger range, and increase the level of anonymity in the paper the algorithmic foundations of differential privacy. It is noted that differential privacy promises that the outcome of a survey will stay the same, whether or not you participate in it. Therefore you don't have any reason not to participate in the survey. You don't have to fear that your data. In this case, you're no speaking habits will be exposed. 
  
In the explaination_of_code document I will be exaplaining the code in simple words so that you can train our own model.
